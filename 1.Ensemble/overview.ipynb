{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "- group of weak learner models can come to gother to form a strong learner\n",
    "- when we said \"weak learners\" we don't mean they weak but they weaker than the new model\n",
    "or in other words the new model have better predictive performance or higer accuracy\n",
    "\n",
    "### why\n",
    "why we need ensemble ?<br>\n",
    "By **combining algorithms**, we can often build models that perform better by meeting in the middle in terms of **bias** and **variance**\n",
    "\n",
    "#### bias\n",
    "- When a model has a high **bias**, this means that means it doesn't do a good job of bending to the data.\n",
    "\n",
    "-High bias typically leads to **underfitting** because the model makes **overly simplistic assumptions**.\n",
    "\n",
    "- **Underfitting** : the model is too simple to capture the underlying patterns in the data, leading to **poor performance on both training and test data**.\n",
    "\n",
    "\n",
    "#### variance\n",
    "- When a model has high **variance**, this means that it changes drastically to meet the needs of every point in our dataset.\n",
    "- refers to the model's sensitivity to fluctuations in the training data. High variance means the model fits the training data closely but fails to generalize to new data, often resulting in overfitting.\n",
    "- **overftting** : the model is too complex and caputre the underlaing patter in the training data very well but can't predicte new value\n",
    "\n",
    "<img src=\"imgs/model complixity.png\" width=\"600px\">\n",
    "<img src=\"imgs/model complixity3.png\" width=\"600px\">\n",
    "<hr>\n",
    "in ensemble we want to find the middel point<br><br>\n",
    "\n",
    "<img src=\"imgs/model complixity2.png\" width=\"600px\" style=\"background-color:white\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method that is used to improve ensemble methods is to introduce randomness into high variance algorithms before they are ensembled together\n",
    "- Bootstrap the data - that is, sampling the data with replacement and fitting your algorithm to the sampled data.<a href=\"https://www.youtube.com/watch?v=Xz0x-8-cgaQ\"> read more about Bootstrap</a>\n",
    "- Subset the features - in each split of a decision tree or with each algorithm used in an ensemble, only a subset of the total possible features are used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST \n",
    "to solve overfiting problem in decision tree,<br> \n",
    "we can create random decision tree base on Subset the features <br>\n",
    "<img src=\"imgs/RANDOM FOREST.png\">\n",
    "\n",
    "and take the most appere value in our example \"whatsapp\"\n",
    "\n",
    "### How Random Forest Works\n",
    "\n",
    "- Training Phase:\n",
    "  - Multiple subsets of the dataset are created using bootstrapping.\n",
    "  - A decision tree is trained on each subset.\n",
    "  - During the training of each tree, a random subset of features is selected at each split.\n",
    "\n",
    "- Prediction Phase:\n",
    "  - For classification, each tree in the forest votes for a class, and the class with the most votes is the final prediction.\n",
    "  - For regression, the average of the predictions from all the trees is the final prediction.\n",
    "\n",
    "### Advantages\n",
    "- Accuracy: Generally provides high accuracy and robustness.\n",
    "- Overfitting: Less prone to overfitting compared to individual decision trees.\n",
    "- Versatility: Can be used for both classification and regression tasks.\n",
    "### Disadvantages\n",
    "- Complexity: More complex and computationally intensive than individual decision trees.\n",
    "- Interpretability: Harder to interpret compared to a single decision tree.\n",
    "Examp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating)\n",
    "**Definition**:\n",
    "Bagging is an ensemble learning technique that aims to improve the stability and accuracy of machine learning models by combining predictions from multiple versions of a base model. It reduces variance and helps prevent overfitting, especially in high-variance models like decision trees.\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "- Bootstrap Sampling: From the original training data, create multiple random subsets with replacement (each subset may contain duplicate samples).\n",
    "- Model Training: Train a separate instance of the base model on each subset. These models will have slight variations due to the different training samples.\n",
    "- Aggregation: For predictions, aggregate the outputs of all models:\n",
    "- Regression: Use the average of predictions.\n",
    "-Classification: Use majority voting.\n",
    "\n",
    "### Why It Works:\n",
    "\n",
    "- Reduces Variance: By averaging predictions from multiple models, bagging reduces the variance of the overall prediction.\n",
    "- Stabilizes Models: It makes models less sensitive to noise and variations in the data, especially effective for high-variance models.\n",
    "\n",
    "### Common Use Case:\n",
    "Bagging is commonly used with decision trees (resulting in models like Random Forests), as they are naturally high-variance models.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "Improves model robustness and accuracy.\n",
    "Reduces overfitting for high-variance models.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "May increase computational cost due to training multiple models.\n",
    "Less effective for low-variance models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Load the dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Parameters\n",
    "n_trees = 100\n",
    "n_samples = X_train.shape[0]\n",
    "n_features = X_train.shape[1]\n",
    "max_features = int(np.sqrt(n_features))\n",
    "\n",
    "# List to store the trained decision trees\n",
    "trees = []\n",
    "\n",
    "# Training phase\n",
    "for _ in range(n_trees):\n",
    "    # Bootstrap sampling\n",
    "    indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    X_sample = X_train[indices]\n",
    "    y_sample = y_train[indices]\n",
    "    \n",
    "    # Random feature selection\n",
    "    features = np.random.choice(n_features, max_features, replace=False)\n",
    "    X_sample = X_sample[:, features]\n",
    "    \n",
    "    # Train a decision tree\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_sample, y_sample)\n",
    "    \n",
    "    # Store the tree and the features used\n",
    "    trees.append((tree, features))\n",
    "\n",
    "# Prediction phase\n",
    "predictions = []\n",
    "for tree, features in trees:\n",
    "    # Use only the selected features\n",
    "    X_test_subset = X_test[:, features]\n",
    "    preds = tree.predict(X_test_subset)\n",
    "    predictions.append(preds)\n",
    "\n",
    "# Aggregate predictions (majority vote)\n",
    "predictions = np.array(predictions)\n",
    "y_pred, _ = mode(predictions, axis=0)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred.flatten())\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import panda as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0986122886681098\n"
     ]
    }
   ],
   "source": [
    "corectpoint = 2\n",
    "allpiont = 8\n",
    "acc = corectpoint/allpiont\n",
    "eq = acc / (1 - acc)\n",
    "# natural log\n",
    "eq = np.log(eq)\n",
    "print(eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating)\n",
    "**Definition**:\n",
    "Bagging is an ensemble learning technique that aims to improve the stability and accuracy of machine learning models by combining predictions from multiple versions of a base model. It reduces variance and helps prevent overfitting, especially in high-variance models like decision trees.\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "- Bootstrap Sampling: From the original training data, create multiple random subsets with replacement (each subset may contain duplicate samples).\n",
    "- Model Training: Train a separate instance of the base model on each subset. These models will have slight variations due to the different training samples.\n",
    "- Aggregation: For predictions, aggregate the outputs of all models:\n",
    "- Regression: Use the average of predictions.\n",
    "-Classification: Use majority voting.\n",
    "\n",
    "### Why It Works:\n",
    "\n",
    "- Reduces Variance: By averaging predictions from multiple models, bagging reduces the variance of the overall prediction.\n",
    "- Stabilizes Models: It makes models less sensitive to noise and variations in the data, especially effective for high-variance models.\n",
    "\n",
    "### Common Use Case:\n",
    "Bagging is commonly used with decision trees (resulting in models like Random Forests), as they are naturally high-variance models.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "Improves model robustness and accuracy.\n",
    "Reduces overfitting for high-variance models.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "May increase computational cost due to training multiple models.\n",
    "Less effective for low-variance models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the base estimator\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Initialize the AdaBoost classifier\n",
    "ada_boost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the AdaBoost classifier\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ada_boost.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Overview\n",
    "\n",
    "**AdaBoost** (Adaptive Boosting) is an ensemble learning technique that focuses on combining **weak learners** (models with slightly better than random performance) to form a **strong classifier**. Each weak learner is trained sequentially, and the algorithm adapts by focusing more on data points that were misclassified by previous models. \n",
    "\n",
    "AdaBoost is particularly effective for binary classification problems, but it can be adapted for multi-class problems as well.\n",
    "\n",
    "## How AdaBoost Works\n",
    "\n",
    "1. **Initialize Weights**:\n",
    "   - Each training sample starts with equal weight.\n",
    "   \n",
    "2. **Train Weak Learners Sequentially**:\n",
    "   - A weak learner (often a decision stump, which is a decision tree with a depth of 1) is trained on the weighted data.\n",
    "   - After each weak learner is trained, AdaBoost evaluates its performance and assigns a **weight to the learner** based on its accuracy. Weak learners with better accuracy get higher weights.\n",
    "\n",
    "3. **Update Sample Weights**:\n",
    "   - Misclassified samples receive increased weights, making them more influential in the training of the next weak learner. This focuses the subsequent weak learners on harder-to-classify examples.\n",
    "   \n",
    "4. **Combine Weak Learners**:\n",
    "   - The final model combines all weak learners using their respective weights to make a prediction. Each weak learner contributes to the final decision according to its weight.\n",
    "\n",
    "5. **Final Prediction**:\n",
    "   - For classification, AdaBoost aggregates the weighted votes of all weak learners and selects the class with the highest total vote.\n",
    "\n",
    "## Key Parameters\n",
    "\n",
    "- **n_estimators**: The number of weak learners (decision stumps) to be trained.\n",
    "- **learning_rate**: Controls the weight applied to each learner. Lower values increase stability but may require more weak learners.\n",
    "  \n",
    "## Advantages of AdaBoost\n",
    "\n",
    "- **Focus on Difficult Cases**: AdaBoost improves on harder-to-classify examples, increasing accuracy.\n",
    "- **Versatility**: It can work with various types of weak learners.\n",
    "- **Good Generalization**: AdaBoost tends to avoid overfitting, especially with simpler weak learners like stumps.\n",
    "\n",
    "## Disadvantages of AdaBoost\n",
    "\n",
    "- **Sensitive to Noisy Data**: Misclassifications can lead to overemphasis on noisy instances.\n",
    "- **Computational Cost**: Training sequentially can be slower than parallel ensemble methods like Bagging.\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "AdaBoost is commonly used in applications such as:\n",
    "- Image and object recognition\n",
    "- Text classification\n",
    "- Financial modeling for risk assessment\n",
    "\n",
    "Here's an example of initializing an AdaBoost model:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate an AdaBoostClassifier with 50 estimators and a learning rate of 1.0\n",
    "adaboost_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Preprocess Data (Binary classification: class 0 vs. class 1)\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "# Convert labels to {-1, 1}\n",
    "y[y == 0] = -1\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Initialize Weights\n",
    "n_samples = X_train.shape[0]\n",
    "weights = np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "# Parameters\n",
    "n_estimators = 50\n",
    "learners = []\n",
    "alphas = []\n",
    "\n",
    "# 4. Iteratively Train Weak Learners\n",
    "for _ in range(n_estimators):\n",
    "    # Train a decision stump\n",
    "    stump = DecisionTreeClassifier(max_depth=1, max_leaf_nodes=2)\n",
    "    stump.fit(X_train, y_train, sample_weight=weights)\n",
    "    \n",
    "    # Predict on training data\n",
    "    stump_pred = stump.predict(X_train)\n",
    "    \n",
    "    # Calculate weighted error\n",
    "    miss = (stump_pred != y_train)\n",
    "    error = np.dot(weights, miss)\n",
    "    \n",
    "    # Compute stump weight (alpha)\n",
    "    alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n",
    "    alphas.append(alpha)\n",
    "    learners.append(stump)\n",
    "    \n",
    "    # Update weights\n",
    "    weights *= np.exp(-alpha * y_train * stump_pred)\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "# 5. Make Predictions\n",
    "def predict(X):\n",
    "    clf_preds = [alpha * clf.predict(X) for clf, alpha in zip(learners, alphas)]\n",
    "    y_pred = np.sign(sum(clf_preds))\n",
    "    return y_pred\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = predict(X_test)\n",
    "\n",
    "# 6. Evaluate the Model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
