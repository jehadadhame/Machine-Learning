{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior** refers to guesses we make before having complete information.<br>\n",
    "**Posterior** refers to guesses we've inferred after the new information has arrived.\n",
    "\n",
    "**bayes theorem** switches from what we know to what we infer <br>\n",
    "\n",
    "## what bayes theorem does ??\n",
    "**Based on information that is known, it can infer other information.<br>\n",
    "It infers the probability of A given R.**\n",
    "<hr>\n",
    "<img src=\"img/bayes.png\" width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula for bayes therom\n",
    "\n",
    "<img src=\"img/FormulaForBayesTherom.png\" width = \"750\">\n",
    "\n",
    "$$\n",
    "P(A \\mid R) = \\frac{P(A) \\cdot P(R \\mid A)}{P(A) \\cdot P(R \\mid A) + P(B) \\cdot P(R \\mid B)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(B \\mid R) = \\frac{P(B) \\cdot P(R \\mid B)}{P(A) \\cdot P(R \\mid A) + P(B) \\cdot P(R \\mid B)}\n",
    "$$\n",
    "\n",
    "agin \n",
    "**Prior probabilities are what we knew before we knew that R occurred.**\n",
    "**Posterior probabilities are what we inferred after we knew that R occurred**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42857142857142855, 0.5714285714285714]\n"
     ]
    }
   ],
   "source": [
    "def naiveBaes(pOfA, pOfB, pOfRgivenA,pOfRgivenB):\n",
    "    pa = pOfA * pOfRgivenA\n",
    "    pb = pOfB * pOfRgivenB\n",
    "    \n",
    "    paGivenR = pa / (pa + pb)\n",
    "    pbGivenR = pb / (pa + pb)\n",
    "    return [paGivenR, pbGivenR]\n",
    "\n",
    "print(naiveBaes(3/5,2/5,1/6,2/6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Key Term              | Definition                                                                                                                                                                                                                           |\n",
    "|-----------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Conditional probability | In probability theory, conditional probability is a measure of the probability of an event occurring given that another event has (by assumption, presumption, assertion, or evidence) occurred.                                    |\n",
    "| Naive assumptions     | The assumption that probabilities are independent.                                                                                                                                                                                  |\n",
    "| Posterior probabilities | Posterior probabilities are what we inferred after we knew that R occurred.                                                                                                                |\n",
    "| Prior probabilities   | Prior probabilities are what we knew before we knew that R occurred.                                                                                                                          |\n",
    "| Sensitivity           | How often a test correctly gets a positive result for the condition that's being tested for (also known as the “true positive” rate). The true-positive recognition rate.                     |\n",
    "| Specificity           | The proportion of truly negative cases that were classified as negative. The true-negative recognition rate.                                                                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Overview\n",
    "\n",
    "**Naive Bayes** is a family of probabilistic classifiers based on **Bayes' Theorem**, which assumes that the features are conditionally independent given the class. This is a \"naive\" assumption because, in most real-world problems, features are often correlated. Despite this simplification, Naive Bayes classifiers often perform surprisingly well, especially in text classification tasks like spam detection.\n",
    "\n",
    "## Bayes' Theorem\n",
    "\n",
    "Bayes' Theorem is a fundamental principle in probability theory, given as:\n",
    "\n",
    "$$\n",
    "P(C|X) = \\frac{P(X|C) P(C)}{P(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( P(C|X) \\) is the posterior probability of class \\( C \\) given the features \\( X \\).\n",
    "- \\( P(X|C) \\) is the likelihood of the features given the class.\n",
    "- \\( P(C) \\) is the prior probability of the class.\n",
    "- \\( P(X) \\) is the evidence, which can be ignored during classification as it is constant for all classes.\n",
    "\n",
    "The **Naive** assumption is that the features are independent, which simplifies the computation of \\( P(X|C) \\) as the product of individual feature probabilities.\n",
    "\n",
    "## Types of Naive Bayes Classifiers\n",
    "\n",
    "- **Gaussian Naive Bayes**: Assumes that the features follow a **Gaussian distribution** (normal distribution). This is suitable for continuous features.\n",
    "- **Multinomial Naive Bayes**: Often used for text classification tasks where the features are **word counts** or frequencies (discrete data). The multinomial distribution models the probability of each feature occurring in a given class.\n",
    "- **Bernoulli Naive Bayes**: Similar to multinomial Naive Bayes but works with binary/boolean features (e.g., presence or absence of a word).\n",
    "\n",
    "## How Naive Bayes Works\n",
    "\n",
    "1. **Training**: \n",
    "   - The classifier calculates the probability of each class based on the training data using Bayes' Theorem.\n",
    "   - For each feature, it computes the probability distribution (Gaussian, multinomial, or Bernoulli) given the class.\n",
    "\n",
    "2. **Prediction**:\n",
    "   - For a new instance, the model calculates the posterior probability for each class based on the given features and selects the class with the highest probability.\n",
    "\n",
    "## Advantages of Naive Bayes\n",
    "\n",
    "- **Simplicity**: It's easy to implement and interpret.\n",
    "- **Efficiency**: It is computationally efficient, especially for high-dimensional data.\n",
    "- **Good for Text Classification**: Works well in text classification tasks like spam detection, sentiment analysis, etc.\n",
    "- **Scalability**: Naive Bayes performs well even with large datasets and many features.\n",
    "\n",
    "## Disadvantages of Naive Bayes\n",
    "\n",
    "- **Naive Assumption**: The assumption of feature independence rarely holds in real-world problems, which can reduce the model's accuracy in some cases.\n",
    "- **Zero Probability Problem**: If a feature value that was not observed in the training data appears in the test data, it can lead to a zero probability. This can be mitigated by **Laplace Smoothing**.\n",
    "\n",
    "## Example: Text Classification\n",
    "\n",
    "Consider a spam email classifier. Features could include the presence of certain words like “free,” “money,” or “offer.” The Naive Bayes classifier would calculate the likelihood of each word given whether an email is spam or not. It then computes the probability for each class (spam or not spam) and chooses the class with the highest probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
