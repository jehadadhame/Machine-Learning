{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Formulas\n",
    "\n",
    "### 1. Probability of All Outcomes\n",
    "$$\n",
    "P(all) = \\prod_{i=1}^n  \\left[ y_i p_i + (1 - y_i) (1 - p_i) \\right]\n",
    "$$\n",
    "\n",
    "### 2. Logarithmic Loss Example  \n",
    "$$\n",
    "- \\log(0.6) - \\log(0.2) - \\log(0.1) - \\log(0.7) = 4.8\n",
    "$$\n",
    "\n",
    "### 3. Cross-Entropy Loss (Binary Classification)  \n",
    "$$\n",
    "\\text{Cross-entropy} = - \\sum_{i=1}^m \\left[ y_i \\ln(p_i) + (1 - y_i) \\ln(1 - p_i) \\right]\n",
    "$$\n",
    "\n",
    "### 4. Cross-Entropy Loss (Multiclass Classification)  \n",
    "$$\n",
    "\\text{Cross-entropy} = - \\sum_{i=1}^n \\sum_{j=1}^m y_{ij} \\ln(p_{ij})\n",
    "$$\n",
    "\n",
    "### 5. Error Function for Binary Classification  \n",
    "$$\n",
    "\\text{Error function} = -\\frac{1}{m} \\sum_{i=1}^m \\left[ (1 - y_i) \\ln(1 - \\hat{y}_i) + y_i \\ln(\\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "### 6. Error Function with Sigmoid Activation  \n",
    "$$\n",
    "E(W, b) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ (1 - y_i) \\ln(1 - \\sigma(W x^{(i)} + b)) + y_i \\ln(\\sigma(W x^{(i)} + b)) \\right]\n",
    "$$\n",
    "\n",
    "### 7. Error Function for Multiclass Classification  \n",
    "$$\n",
    "\\text{Error function} = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^n y_{ij} \\ln(\\hat{y}_{ij})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of neural networks and their optimization, there are several other important formulas and concepts. Here are a few key ones to consider adding:\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Gradient Descent Update Rule**  \n",
    "The basic formula for updating weights in gradient descent:  \n",
    "$$\n",
    "w \\leftarrow w - \\eta \\frac{\\partial E}{\\partial w}\n",
    "$$  \n",
    "Where:  \n",
    "- \\( w \\): Weight parameter  \n",
    "- \\( \\eta \\): Learning rate  \n",
    "- \\( \\frac{\\partial E}{\\partial w} \\): Gradient of the loss function with respect to the weight  \n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Chain Rule for Backpropagation**  \n",
    "The chain rule is used to compute gradients during backpropagation:  \n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}\n",
    "$$  \n",
    "Where:  \n",
    "- \\( a \\): Activation  \n",
    "- \\( z \\): Weighted sum before activation  \n",
    "- \\( w \\): Weight  \n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Sigmoid Activation Function**  \n",
    "The sigmoid activation function is commonly used in binary classification tasks:  \n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$  \n",
    "Its derivative is:  \n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "### 11. **ReLU (Rectified Linear Unit) Activation Function**  \n",
    "The ReLU activation function is used for hidden layers:  \n",
    "$$\n",
    "f(z) = \\max(0, z)\n",
    "$$  \n",
    "Its derivative:  \n",
    "$$\n",
    "f'(z) =\n",
    "\\begin{cases} \n",
    "1 & \\text{if } z > 0 \\\\\n",
    "0 & \\text{if } z \\leq 0 \n",
    "\\end{cases}\n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "### 12. **Softmax Function**  \n",
    "For multiclass classification, the softmax function converts raw scores into probabilities:  \n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}\n",
    "$$  \n",
    "Where \\( z_i \\) is the \\( i \\)-th raw score, and \\( n \\) is the number of classes.\n",
    "\n",
    "---\n",
    "\n",
    "### 13. **Weight Initialization**  \n",
    "To prevent vanishing or exploding gradients, weights are often initialized using:  \n",
    "**He Initialization (for ReLU):**  \n",
    "$$\n",
    "w \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{n}})\n",
    "$$  \n",
    "**Xavier Initialization (for sigmoid/tanh):**  \n",
    "$$\n",
    "w \\sim \\mathcal{N}(0, \\sqrt{\\frac{1}{n}})\n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "### 14. **Mean Squared Error (MSE)**  \n",
    "An alternative loss function used in regression tasks:  \n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}_i - y_i)^2\n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "### 15. **L2 Regularization (Weight Decay)**  \n",
    "To prevent overfitting:  \n",
    "$$\n",
    "\\text{Regularized Loss} = E(W, b) + \\lambda \\sum_{i=1}^n w_i^2\n",
    "$$  \n",
    "Where \\( \\lambda \\) is the regularization parameter.\n",
    "\n",
    "---\n",
    "\n",
    "These formulas are central to understanding how neural networks work, how they are trained, and how they make predictions. If you're exploring a specific area, such as optimization or activation functions, let me know, and I can suggest more!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
