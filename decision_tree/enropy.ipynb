{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy \n",
    "how many freedom does a particle have to move around?\n",
    "<img src=\"images/Entropy.png\" width=400>\n",
    "<table style=\"width:50%; border-collapse: collapse; text-align: center; font-family: Arial, sans-serif;\">\n",
    "    <tr style=\"background-color: #f2f2f2; font-weight: bold;\">\n",
    "        <td colspan=\"3\" style=\"padding: 10px; border: 1px solid #dddddd;\">Entropy</td>\n",
    "    </tr>\n",
    "    <tr style=\"font-weight: bold;\">\n",
    "        <td style=\"padding: 10px; border: 1px solid #dddddd;\">High</td>\n",
    "        <td style=\"padding: 10px; border: 1px solid #dddddd;\">Medium</td>\n",
    "        <td style=\"padding: 10px; border: 1px solid #dddddd;\">Low</td>\n",
    "    </tr>\n",
    "    <tr style=\"background-color: #f9f9f9; font-weight: bold;\">\n",
    "        <td style=\"padding: 10px; border: 1px solid #dddddd;\">Low Knowledge</td>\n",
    "        <td style=\"padding: 10px; border: 1px solid #dddddd;\">Medium Knowledge</td>\n",
    "        <td style=\"padding: 10px; border: 1px solid #dddddd;\">High Knowledge</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{probability} = -\\frac{m}{m+n}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probablity(m, n):\n",
    "    return m / (m+n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Formula 1\n",
    "**Entropy** measures the level of uncertainty or impurity in a dataset. In decision trees, entropy helps determine how to split the data to reduce this uncertainty.\n",
    "$$\n",
    "\\text{Entropy} = -\\frac{m}{m+n} \\log_2\\left( \\frac{m}{m+n} \\right) - \\frac{n}{m+n} \\log_2\\left( \\frac{n}{m+n} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "**OR** \n",
    "$$\n",
    "\\text{Entropy} = -p_1 \\log_2(p_1) - p_2 \\log_2(p_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Entropy} = -p_1 \\log_2(p_1) - p_2 \\log_2(p_2) - \\dots - p_n \\log_2(p_n) \n",
    "$$\n",
    "$$\n",
    "= -\\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to calculate entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(probabilities):\n",
    "    ent = 0\n",
    "    for prob in probabilities:\n",
    "        ent += - prob * np.log2(prob)\n",
    "        print(prob, - prob * np.log2(prob))\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: Equal Distribution** <br>\n",
    "Consider a dataset with an equal number of positives and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n",
      "0.5 0.5\n",
      "Entropy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Probabilities for equal distribution\n",
    "probabilities = [0.5, 0.5]\n",
    "entropy = calculate_entropy(probabilities)\n",
    "print(f'Entropy: {entropy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: Unequal Distribution\n",
    "\n",
    "Now, a dataset with 9 positives and 1 negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9 0.13680278410054494\n",
      "0.1 0.33219280948873625\n",
      "Entropy: 0.4689955935892812\n"
     ]
    }
   ],
   "source": [
    "# Probabilities for unequal distribution\n",
    "probabilities = [0.9, 0.1]\n",
    "entropy = calculate_entropy(probabilities)\n",
    "print(f'Entropy: {entropy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: 0.3333333333333333\n",
      "Probability: 0.6666666666666666\n",
      "0.3333333333333333 0.5283208335737187\n",
      "0.6666666666666666 0.38997500048077083\n",
      "Entropy: 0.9182958340544896\n"
     ]
    }
   ],
   "source": [
    "numbers = [1, 2]\n",
    "m = sum(numbers)\n",
    "probabilities = []\n",
    "for num in numbers:\n",
    "    probabilities.append(num / m)\n",
    "for prop in probabilities:\n",
    "    print(f'Probability: {prop}')\n",
    "entropy = calculate_entropy(probabilities)\n",
    "print(f'Entropy: {entropy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
