{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data.\n",
    "data = np.asarray(pd.read_csv('data/data.csv', header=None))\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:, :2]\n",
    "y = data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/overview.png\">\n",
    "\n",
    "## outline\n",
    "- Testing your models\n",
    "- Confusion matrix\n",
    "- Accuracy\n",
    "- When accuracy won't work\n",
    "- False negatives and positives\n",
    "- Precision and recall\n",
    "- F1 score\n",
    "- F-beta score\n",
    "- ROC curve\n",
    "- Classification in sklearn\n",
    "- Regression in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testin your model \n",
    "using train_test_split function to split your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train test split to split your data \n",
    "# Use a test size of 25% and a random state of 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix Overview\n",
    "\n",
    "- A **Confusion Matrix** is a performance evaluation tool for classification models.\n",
    "- It displays the counts of:\n",
    "  - **True Positives (TP)**: Correctly predicted positive cases\n",
    "  - **True Negatives (TN)**: Correctly predicted negative cases\n",
    "  - **False Positives (FP)**: Incorrectly predicted positive cases\n",
    "  - **False Negatives (FN)**: Incorrectly predicted negative cases\n",
    "- Typically represented as a 2x2 matrix for binary classification problems.\n",
    "- Useful for calculating metrics like **accuracy**, **precision**, **recall**, and **F1-score**.\n",
    "- Provides insights into model performance, especially for **imbalanced datasets**.\n",
    "- This is a table that will describe the performance of a model.\n",
    "\n",
    "<img src=\"imgs/Confusion Matrix.png\"><br>\n",
    "\n",
    "- Type 1 Error (Error of the first kind, or False Positive): In the medical example, this is when we misdiagnose a healthy patient as sick.\n",
    "- Type 2 Error (Error of the second kind, or False Negative): In the medical example, this is when we misdiagnose a sick patient as healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "**Accuracy** is the answer to the question, \"Out of all the patients, how many did we classify correctly?\"\n",
    "## Confusion Matrix and Accuracy Formula\n",
    "\n",
    "|                | Predicted: Spam (Positive) | Predicted: Not Spam (Negative) |\n",
    "|----------------|----------------------------|---------------------------------|\n",
    "| **Actual: Spam (Positive)**   | True Positive (TP)           | False Negative (FN)           |\n",
    "| **Actual: Not Spam (Negative)** | False Positive (FP)          | True Negative (TN)            |\n",
    "\n",
    "### Accuracy Formula\n",
    "\n",
    "The **accuracy** of a model is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **TP** = True Positives\n",
    "- **TN** = True Negatives\n",
    "- **FP** = False Positives\n",
    "- **FN** = False Negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Accurac Won't Work \n",
    "in this example : <br>\n",
    "\n",
    "<img src=\"imgs/credit card faraud example.png\">\n",
    "\n",
    "our model accuracy = 99.83\n",
    "but can't catch any bad one's\n",
    "\n",
    "how can we solve this problem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Negatives and Positives\n",
    "another question in spam email classifier which is worst **false positive** or **false negative** in other words find some spame email in your inbox or lose important email because the model thik it's spam ? \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision\n",
    "the precision is the answer of this question :\n",
    "\"**out of all point predicted to be positive how many of them are actually positive?**\"\n",
    "<br>\n",
    "or in our example : \n",
    "<br>\n",
    "\"out of all email that the model predicte they spame how many of them are spam ? \"\n",
    "\n",
    "![Precision Formula](imgs/Precision.png)\n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall\n",
    "the recall is the answer of this question:\n",
    "\"**out of points that are labeled as positive how many of them correctly predicted as positive?**\"  \n",
    "or in spam classifer:   \n",
    "\"out of spam email how many of them did the model catch?\"  \n",
    "![recall](imgs/Recall.png)  \n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$  \n",
    "- - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score\n",
    "\n",
    "if we want to combine the tow number what should we do?  \n",
    "Accuracy can't help us.  \n",
    "$$\n",
    "\\text{Harmonic Mean} = \\frac{2xy}{x + y}\n",
    "$$\n",
    "\n",
    "f1 score = hermonicMean(precision, recall)\n",
    "$$\n",
    "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "F_{\\beta} = (1 + \\beta^2) \\times \\frac{\\text{Precision} \\times \\text{Recall}}{(\\beta^2 \\times \\text{Precision}) + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "- The smaller the beta, the more towards precision that we get.\n",
    "- The larger the beta, the more towards recall that we get.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_true, y_pred, average='weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
